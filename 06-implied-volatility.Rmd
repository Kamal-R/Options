<!-- Each chapter is set to compile separately - include "global" set-up -->

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

if ( !require("tidyverse") )   { install.packages("tidyverse") };    require(tidyverse)
if ( !require("kableExtra") )  { install.packages("kableExtra") };   require(kableExtra)
if ( !require("formattable") ) { install.packages("formattable") };  require(formattable)

options(kableExtra.html.bsTable = T)
options(kableExtra.auto_format = FALSE)
```

``` {r Set-Global-Chapter-Variables, echo = FALSE}
plot_bg <- "#FFFFFF" # "#191919"
plot_fg <- "#000000" # "#929292"
plot_fg_alt <- "#969696" # ??
label_font_size <- 7.5
```

# Implied Volatility And Vega {#Implied-Vol-Vega}

<!-- TAKE 3: A Success? -->

Implied volatility is strange because it is *unobserable* - we cannot look at a price chart and know what the implied volatility is. We have to use a statistical model to infer the implied volatility of an option. But what is implied volatility measuring? At a high level, it tells us the plausible range of values a stock can take. It is quoted by online brokers as the percentage of the current price the stock is likely to move in the next year.

The 


<!--

TAKE 2: FAILED??

The first thing to know about implied volatility (or *sigma*) is that you can't read off a chart. It has to be estimated by a mathematical model. The data you use, the modeling assumptions you make, and the tool you use to perform the estimation all play a roll in the the estimate of *sigma* you produce. Investors use many different techniques to estimate *sigma*, but I imagine they mostly produce the same results. If for no other reason than if a particular was wrong too often, they would make lots of poor trades and simply go bankrupt.

## Implied Volatilty {#Implied-Vol}

There are two major volatility measurements - implied volatility and historical volatility. Historical volatility is backward-looking - it is calculated using the last 12 months of stock market data. Implied volatility is forward-looking - it is the market's opinion of how volatile the stock market will be over the next 12 months. Implied volatility is often higher than historical volatility, so some trading strategies do exploit the difference between them to try and make money. I'm going to focus on implied volatility for now, and we'll get to options strategies that use it later. 

The key to understanding implied volatility is that we have make a modeling assumption that gives it meaning. Options are continuously priced by the market, and their value at any point in time depends on how likely their stock is to be above its strike price. That means that to price the option, we have to make an assumption about what governs the path of XYZ's future prices. 

I need two change-of-measure results, both obtained via Girsanov's theorem.

**COMMENT OUT**
Source: https://pdfs.semanticscholar.org/2315/bbe439680db469a8f0519bebd9b07c68f384.pdf
Under P: If dSt = mu dt + simga dWt is an arithmetic Brownian motion, St ~ N( S0 + mu*t, sigma*sqrt(t) )
Under Q: If dSt = exp(-rt) sigma dWt*, St ~ N( S0*exp(rt),  exp(rt)*sigma*sqrt( (1-exp(2rt)) / 2r ) )

Source: https://en.wikipedia.org/wiki/Geometric_Brownian_motion
Under P: If dSt = St mu dt + St sigma dWt is a geometric Brownian motion, 
  St ~ N( S0*exp(mu*t), sqrt(S0^2 * exp(2*mu*t) * (exp(sigma^2*t)-1 ) ) )
Under Q: By Girsanov's theorem - under Q, with r = mu - sigma^2 / 2, 
  St ~ N( S0*exp(r*t), sqrt(S0^2 * exp(2*r*t) * (exp(sigma^2*t)-1 ) ) )
**COMMENT OUT**

I'm going walk through two possible assumptions about the model that governs XYZ's future prices. The first is they are governed by a Brownian motion, and the second is that they are governed by a Geometric Brownian motion. If the market is pricing an implied volatility of 15 and I assume that XYZ's future stock price is governed by a Brownian motion, 

If you've have the misfortune of taking statistics courses before, you might remember the *normal* distribution. The Black-Scholes model assumes stock prices have a *lognormal* distribution. It might help build your intuition to see the shape of these distributions. 

```{r Normal-Example, echo = FALSE, out.width = "80%", fig.align = "center", message = FALSE}
data_std <- data.frame( x = 100+seq(from = 3*-4, to = 3*4, by = 0.1), 
                    y = dnorm(x = seq(from = 100 + 3*-4, to = 100 + 3*4, by = 0.1), 
                              mean = 100, sd = 3) )

ggplot( data = data_std, aes(x = x, y = y) ) + 
  geom_line() + 
  # Vertical lines for empirical rule 
  geom_segment(aes(x = x, xend = x, y = min(data_std$y), yend = y),
               data = data_std[ data_std$x %in% (100+3*c(-3,-2,-1,1,2,3)), ],
               lty = "dashed", size = 0.5) +
  # Horizonal line for 1 standard deviation = 68% 
  geom_ribbon(data = data.frame( x = seq(from = 100+3*-1, to = 100+3*1, by = 0.1),
                                 y = dnorm(x = seq(from = 100+3*-1, to = 100+3*1, by = 0.1), 
                                           mean = 100+0, sd = 3*1) ), 
              aes(ymin = 0, ymax = y), fill=plot_fg, alpha="0.5") + 
  geom_segment(aes(x = 100+3*-1, xend = 100+3*1, y = 0.175/3, yend = 0.175/3), 
               lty = "dashed", size = 0.2, col = "black",
               arrow = arrow(ends = "both",
                             length = unit(0.15, "inches"))) + 
  geom_label(aes(label = "68%", x = 100+0, y = 0.175/3, size = label_font_size), col = "black", 
             label.size = 0, label.padding = unit(0.1,"lines"), 
             fill = plot_bg) +
  # Horizonal line for 2 standard deviations = 95% 
  geom_ribbon(data = data.frame( x = seq(from = 100+3*-2, to = 100+3*-1, by = 0.1),
                                 y = dnorm(x = seq(from = 100+3*-2, to = 100+3*-1, by = 0.1), 
                                           mean = 100+0, sd = 3) ), 
              aes(ymin = 0, ymax = y), fill=plot_fg_alt, alpha="0.5") + 
  geom_ribbon(data = data.frame( x = seq(from = 100+3*1, to = 100+3*2, by = 0.1),
                                 y = dnorm(x = seq(from = 100+3*1, to = 100+3*2, by = 0.1), 
                                           mean = 100+0, sd = 3) ), 
              aes(ymin = 0, ymax = y), fill=plot_fg_alt, alpha="0.5") + 
  geom_segment(aes(x = 100+3*-2, xend = 100+3*2, 
                   y = 0.05/3, yend = 0.05/3),
               lty = "dashed", size = 0.2, 
               arrow = arrow(ends = "both",
                             length = unit(0.15, "inches"))) +
  geom_label(aes(label = "95%", x = 100+0, y = 0.05/3, size = label_font_size), 
             col = "black", label.size = 0, label.padding = unit(0.1,"lines"), 
             fill = plot_bg) +
  scale_size_identity() + 
  scale_y_continuous(breaks = NULL) + 
  scale_x_continuous( breaks = 100+3*c(-3,-2,-1,0,1,2,3) ) +
  labs(title = "Company XYZ's Future Price",
       x = "Future Price",
       y = "Density") + 
  theme_classic() + 
  theme(plot.title = element_text(hjust = 0.5, face="bold", 
                                  margin = margin(10, 0, 10, 0),
                                  colour = plot_fg),
        legend.justification=c(0,1), 
        legend.position=c(0,1), 
        legend.title = element_blank(),
        legend.key = element_rect(fill = plot_bg, color = plot_bg),
        legend.background = element_rect(fill = plot_bg, color = plot_bg),
        legend.box.background = element_rect(fill = plot_bg, color = plot_bg),
        legend.text = element_text(colour = plot_fg),
        text = element_text(size=16), 
        panel.border = element_blank(), 
        panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(), 
        panel.background = element_rect(fill = plot_bg, color = plot_bg),
        plot.background = element_rect(fill = plot_bg, color = plot_bg),
        axis.line = element_line(colour = plot_bg),
        axis.title.x = element_text(margin = unit(c(4, 0, 0, 0), "mm"), 
                                    colour = plot_fg),
        axis.title.y = element_text(margin = unit(c(0, 4, 0, 0), "mm"), 
                                    angle = 90, colour = plot_fg) )
```

In chapter 4, I looked at the *delta* and *gamma* of a call option on Company XYZ. The call had a strike of \$100, and expired in 6 months. If this picture is telling us the market's view of what XYZ's price will be in 6 months when my call expires, it has a ton of information in it. How do we read this picture? 

It's easy. The values along the bottom are the future prices that XYZ can take, and the shaded area under the curve adds up to 100%. If we want to know how likely a range of prices is in the future, all we have to do is eyeball how much of the area under the curve it fills in. I've drawn in two examples of how to do this.  
  
  1. 68% of the shaded area is between \$97 and \$103, so the market is telling us it sees a 68% chance that XYZ's future price will fall between \$97 and \$103.
  2. 95% of the shaded area is between \$94 and \$106, so the market is telling us it sees a 95% chance that XYZ's future price will fall between \$94 and \$106.

The picture tells us even more. The bell curve is centered at \$100, which means the market is predicting XYZ's average future price will be \$100. It also peaks at \$100, which means the market is predicting that XYZ's most likely price is \$100. Just by eyeballing the curve, we can see it slopes gently down as it moves away from \$100. The curve doesn't spike up at any price, so the market isn't seeing an outsized chance that XYZ does very poorly or very well. Those spikes higher often happen in the market, perhaps because investors have to grapple with the chances of unlikely but impactful events, like large earnings beats or misses. 

But that's enough about the curve. What's the implied volatility? For the bell curve, it's half the distance between the dollar values that cover 68% of the curve. Those values are \$97 and \$103, which are \$6 apart. Half of that is \$3, so the implied volatility is \$3. If the values were \$98 and \$102, they would be \$4 apart and the implied volatility would be \$2. 

What does the implied volatility tell us? I think of it un a couple of different ways. The first way is as as a multiplier on the range of a stock's plausible future prices. Since the implied volatility is \$3, the market is saying that the stock will end up between \$97 and \$103 over 60% of the time. It will end up between \$94 and \$106 over 90% of the time. If the implied volatility is \$2, the stock will end up between \$98 and \$102 over 60% of the time and between \$96 and \$104 over 90% of the time. It really does function as multiplier on the range of plausible future prices.

I've said implied volatility is \$3, which it is. But your online broker (and everyone else) reports it as an annual percentage change. XYZ was at \$100 when I bought the call, so the percentage change is actually how many dollars it goes up or down by. I don't have to do anything to convert to percent, but I bought a 6 month call. To make it an annual (12 month) rate, I have to double it. So the implied volatility of XYZ's call is 6. That's extremely low, as you'll see in the next section. 

One way to think of implied volatility is as a multiplier on the range of a stock's plausible future prices. Two extreme examples should make this very clear. The first example says the \$100 strike call on XYZ has an extremely low implied volatility of 1 (or \$0.50, in dollars per 6 months). If it does, the market thinks XYZ will end up between \$99.50 and \$100.50 68% of the time and between \$99 and \$101 95% of the time. The second example says the \$100 strike call on XYZ has a much higher implied volatility of 10 (or \$5, in dollars per 6 months). This says the market thinks XYZ will end up between \$95 and \$105 68% of the time, and between \$90 and \$110 95% of the time. 

**COMMENT OUT** INSERT PICTURE **COMMENT OUT**

Let me ask you: Which one of those \$100 calls would you pay more for? The one with an implied volatility of 1 or the implied volatility of 10? The best outcome I can reasonably expect to get when the implied volatility is 1 is that XYZ goes up to \$101. When the implied volatility is at 10, I can reasonably expect XYZ to reach \$110. I can only the cost of the call, but I make every dollar it goes over its strike price of \$100 by (after paying for the call). If I can make \$1 with the first call and \$10 for the second, guess which one I'm willing to pay less for? It's the first one, with an implied volatility of 1.

We should take a quick look at the lognormal distribution. I'll match the mean and implied volatility of the 

## Volatility Skew {#Vol-Skew}



## Vega
-->





<!--

TAKE 1: FAILED

What is *implied volatility*? We need to add some statistics to our toolbelt - namely, the normal distribution - to answer that. You might have heard of it by it's other name - the "bell curve". If you've had the misfortune of taking a statistics class, you might remember it from the countless hours you spent using z-tables and the empirical rule. 

### The Standard Normal Distribution 

We're going to start our exploration into the normal distribution by drawing some pictures. Our first picture shows the *standard* normal distribution, with the empirical rule drawn on over the bell curve. 

```{r Standard-Normal-Distribution, echo = FALSE, out.width = "80%", fig.align = "center", message = FALSE}
data_std <- data.frame( x = seq(from = -4, to = 4, by = 0.1), 
                    y = dnorm(x = seq(from = -4, to = 4, by = 0.1), 
                              mean = 0, sd = 1) )

ggplot( data = data_std, aes(x = x, y = y) ) + 
  geom_line() + 
  # Vertical lines for empirical rule 
  geom_segment(aes(x = x, xend = x, y = min(data_std$y), yend = y),
               data = data_std[ data_std$x %in% c(-3,-2,-1,1,2,3), ],
               lty = "dashed", size = 0.5) +
  # Horizonal line for 1 standard deviation = 68% 
  geom_ribbon(data = data.frame( x = seq(from = -1, to = 1, by = 0.1),
                                 y = dnorm(x = seq(from = -1, to = 1, by = 0.1), 
                                           mean = 0, sd = 1) ), 
              aes(ymin = 0, ymax = y), fill=plot_fg, alpha="0.5") + 
  geom_segment(aes(x = -1, xend = 1, y = 0.175, yend = 0.175), 
               lty = "dashed", size = 0.2, col = "black",
               arrow = arrow(ends = "both",
                             length = unit(0.15, "inches"))) + 
  geom_label(aes(label = "68%", x = 0, y = 0.175, size = label_font_size), col = "black", 
             label.size = 0, label.padding = unit(0.1,"lines"), 
             fill = plot_bg) +
  # Horizonal line for 2 standard deviations = 95% 
  geom_ribbon(data = data.frame( x = seq(from = -2, to = -1, by = 0.1),
                                 y = dnorm(x = seq(from = -2, to = -1, by = 0.1), 
                                           mean = 0, sd = 1) ), 
              aes(ymin = 0, ymax = y), fill=plot_fg_alt, alpha="0.5") + 
  geom_ribbon(data = data.frame( x = seq(from = 1, to = 2, by = 0.1),
                                 y = dnorm(x = seq(from = 1, to = 2, by = 0.1), 
                                           mean = 0, sd = 1) ), 
              aes(ymin = 0, ymax = y), fill=plot_fg_alt, alpha="0.5") + 
  geom_segment(aes(x = -2, xend = 2, y = 0.05, yend = 0.05),
               lty = "dashed", size = 0.2, 
               arrow = arrow(ends = "both",
                             length = unit(0.15, "inches"))) +
  geom_label(aes(label = "95%", x = 0, y = 0.05, size = label_font_size), col = "black",
             label.size = 0, label.padding = unit(0.1,"lines"), 
             fill = plot_bg) +
  scale_size_identity() + 
  scale_y_continuous(breaks = NULL) + 
  scale_x_continuous(breaks = c(-3,-2,-1,0,1,2,3) ) +
  labs(title = "Standard Normal Distribution",
       x = "Z Score",
       y = "Density") + 
  theme_classic() + 
  theme(plot.title = element_text(hjust = 0.5, face="bold", 
                                  margin = margin(10, 0, 10, 0),
                                  colour = plot_fg),
        legend.justification=c(0,1), 
        legend.position=c(0,1), 
        legend.title = element_blank(),
        legend.key = element_rect(fill = plot_bg, color = plot_bg),
        legend.background = element_rect(fill = plot_bg, color = plot_bg),
        legend.box.background = element_rect(fill = plot_bg, color = plot_bg),
        legend.text = element_text(colour = plot_fg),
        text = element_text(size=16), 
        panel.border = element_blank(), 
        panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(), 
        panel.background = element_rect(fill = plot_bg, color = plot_bg),
        plot.background = element_rect(fill = plot_bg, color = plot_bg),
        axis.line = element_line(colour = plot_bg),
        axis.title.x = element_text(margin = unit(c(4, 0, 0, 0), "mm"), 
                                    colour = plot_fg),
        axis.title.y = element_text(margin = unit(c(0, 4, 0, 0), "mm"), 
                                    angle = 90, colour = plot_fg) )
```

The normal distribution is a *probability distribution* that governs how likely different future outcomes are. Any future event with more than one outcome can be described by a probability distribution. Typical examples are flipping a coin (which has two outcomes - heads or tails) and rolling a dice (which has six outcomes - 1, 2, 3, 4, 5, or 6). A probability distribution governs how likely those outcomes are. If we are flipping a fair coin, we expect to land heads half of the time and to land tails the other half of the time. If we were rolling a fair dice, we would expect to roll each number 1/6^th^ of the time. 

We have good intuition for flipping coins and rolling dice, and the empirical rule helps us gain intuition when it applies. It says 68% of the future outcomes will be between -1 and 1, and 95% will be between -2 and 2. We didn't know what the future outcome would be when we rolled the dice, and we don't know now under the empirical rule. But when we say that the standard normal distribution *governs* how likely future events are, we know how likely seeing a number between -1 and 1 is, and how likely seeing a number between -2 and 2 is. That's better than nothing. 

That's all a probability distribution can give us. We don't know what's going to happen in the future, so the best we can do is try to describe how likely we think future events are. We might even be wrong - we could think the empirical rule is a reasonable approximation of the future, only to find that 35% of future outcomes are between -1 and 1 (instead of 68%) and 60% are between -2 and 2 (instead of 95%). If that happened and we use the empirical rule to help us reason about the future, we could make a very bad decision. 

In some ways, this is unavoidable - we don't know what's going to happen, so we have to make an assumption that helps us reason about the future. Assumptions that are wrong can still be helpful. We just have to be mindful about how they're wrong, and factor their shortcomings into our thinking when we're making our decision. This is an issue we'll have to confront for *implied volatility*. 

Finally, I'd argue that being precise and putting a number on our forecasts is better than using vague words like "more likely" or "less likely". If we use actual numbers, we can track how accurate we are. A simple way to track forecast accuracy is if events happen as frequently as we believe they will. For example, an event that we believe will happen 40% of them time should happen 40% of the time. A more sophisticated way to track forecasting accuracy is the Brier Score, beautifully described in Philip Tetlock's book [Superforecasting](https://www.amazon.com/Superforecasting-Science-Prediction-Philip-Tetlock/dp/0804136718). 


### The Normal Distribution 

We've seen the standard normal distribution. It is centered at 0 and says 68% of future outcomes are between -1 and 1. We can center the normal distribution at any number - if we center it at 5, 68% of its values will be between 4 and 6 and 95% of its values will be between 3 and 7. We call the value the normal distribution is centered at its *mean*.

```{r Normal-Mean-5, echo = FALSE, out.width = "80%", fig.align = "center", message = FALSE}
data_std <- data.frame( x = seq(from = 5-4, to = 5+4, by = 0.1), 
                    y = dnorm(x = seq(from = 5-4, to = 5+4, by = 0.1), 
                              mean = 5, sd = 1) )

ggplot( data = data_std, aes(x = x, y = y) ) + 
  geom_line() + 
  # Vertical lines for empirical rule 
  geom_segment(aes(x = x, xend = x, y = min(data_std$y), yend = y),
               data = data_std[ data_std$x %in% (5 + c(-3,-2,-1,1,2,3)), ],
               lty = "dashed", size = 0.5) +
  # Horizonal line for 1 standard deviation = 68% 
  geom_ribbon(data = data.frame( x = seq(from = 5-1, to = 5+1, by = 0.1),
                                 y = dnorm(x = seq(from = 5-1, to = 5+1, by = 0.1), 
                                           mean = 5, sd = 1) ), 
              aes(ymin = 0, ymax = y), fill=plot_fg, alpha="0.5") + 
  geom_segment(aes(x = 5-1, xend = 5+1, y = 0.175, yend = 0.175), 
               lty = "dashed", size = 0.2, col = "black",
               arrow = arrow(ends = "both",
                             length = unit(0.15, "inches"))) + 
  geom_label(aes(label = "68%", x = 5, y = 0.175, size = label_font_size), col = "black", 
             label.size = 0, label.padding = unit(0.1,"lines"), 
             fill = plot_bg) +
  # Horizonal line for 2 standard deviations = 95% 
  geom_ribbon(data = data.frame( x = seq(from = 5-2, to = 5-1, by = 0.1),
                                 y = dnorm(x = seq(from = 5-2, to = 5-1, by = 0.1), 
                                           mean = 5, sd = 1) ), 
              aes(ymin = 0, ymax = y), fill=plot_fg_alt, alpha="0.5") + 
  geom_ribbon(data = data.frame( x = seq(from = 5+1, to = 5+2, by = 0.1),
                                 y = dnorm(x = seq(from = 5+1, to = 5+2, by = 0.1), 
                                           mean = 5, sd = 1) ), 
              aes(ymin = 0, ymax = y), fill=plot_fg_alt, alpha="0.5") + 
  geom_segment(aes(x = 5-2, xend = 5+2, y = 0.05, yend = 0.05),
               lty = "dashed", size = 0.2, 
               arrow = arrow(ends = "both",
                             length = unit(0.15, "inches"))) +
  geom_label(aes(label = "95%", x = 5, y = 0.05, size = label_font_size), col = "black",
             label.size = 0, label.padding = unit(0.1,"lines"), 
             fill = plot_bg) +
  scale_size_identity() + 
  scale_y_continuous(breaks = NULL) + 
  scale_x_continuous(breaks = 5 + c(-3,-2,-1,0,1,2,3) ) +
  labs(title = "Normal Distribution - Centered at 5",
       x = "Z Score",
       y = "Density") + 
  theme_classic() + 
  theme(plot.title = element_text(hjust = 0.5, face="bold", 
                                  margin = margin(10, 0, 10, 0),
                                  colour = plot_fg),
        legend.justification=c(0,1), 
        legend.position=c(0,1), 
        legend.title = element_blank(),
        legend.key = element_rect(fill = plot_bg, color = plot_bg),
        legend.background = element_rect(fill = plot_bg, color = plot_bg),
        legend.box.background = element_rect(fill = plot_bg, color = plot_bg),
        legend.text = element_text(colour = plot_fg),
        text = element_text(size=16), 
        panel.border = element_blank(), 
        panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(), 
        panel.background = element_rect(fill = plot_bg, color = plot_bg),
        plot.background = element_rect(fill = plot_bg, color = plot_bg),
        axis.line = element_line(colour = plot_bg),
        axis.title.x = element_text(margin = unit(c(4, 0, 0, 0), "mm"), 
                                    colour = plot_fg),
        axis.title.y = element_text(margin = unit(c(0, 4, 0, 0), "mm"), 
                                    angle = 90, colour = plot_fg) )
```

We've only seen normal distributions where 68% of the values are within a distance of 1 from the center, and 95% of the values have been within a distance of 2 from the center. Distance tells us how far we are, without paying attention to sign. For example, the standard normal distribution is centered at 0, and 68% of the values are between -1 and 1. We can move from 0 to 1 or from 0 to -1, and only have moved by 1. So, 68% of its values are within a distance of 1 from its center. For the normal distribution centered at 5, 68% of its values are between 4 and 6. We can move from 5 to 4 or from 5 to 6 and only have moved by 1. So 68% of its values are as well. 

We can generalize the normal distribution one more way, by saying that 68% of the values have to within a distance of any number from its center. Let's look at the standard normal distribution, but change it to make that distance 3 instead of 1. This will make 68% of the values be between -3 and 3, and 95% of the values be between -6 and 6. 

```{r Normal-StdDev-3, , echo = FALSE, out.width = "80%", fig.align = "center", message = FALSE}
data_std <- data.frame( x = seq(from = 3*-4, to = 3*4, by = 0.1), 
                    y = dnorm(x = seq(from = 3*-4, to = 3*4, by = 0.1), 
                              mean = 0, sd = 3) )

ggplot( data = data_std, aes(x = x, y = y) ) + 
  geom_line() + 
  # Vertical lines for empirical rule 
  geom_segment(aes(x = x, xend = x, y = min(data_std$y), yend = y),
               data = data_std[ data_std$x %in% (3*c(-3,-2,-1,1,2,3)), ],
               lty = "dashed", size = 0.5) +
  # Horizonal line for 1 standard deviation = 68% 
  geom_ribbon(data = data.frame( x = seq(from = 3*-1, to = 3*1, by = 0.1),
                                 y = dnorm(x = seq(from = 3*-1, to = 3*1, by = 0.1), 
                                           mean = 0, sd = 3*1) ), 
              aes(ymin = 0, ymax = y), fill=plot_fg, alpha="0.5") + 
  geom_segment(aes(x = 3*-1, xend = 3*1, y = 0.175/3, yend = 0.175/3), 
               lty = "dashed", size = 0.2, col = "black",
               arrow = arrow(ends = "both",
                             length = unit(0.15, "inches"))) + 
  geom_label(aes(label = "68%", x = 0, y = 0.175/3, size = label_font_size), col = "black", 
             label.size = 0, label.padding = unit(0.1,"lines"), 
             fill = plot_bg) +
  # Horizonal line for 2 standard deviations = 95% 
  geom_ribbon(data = data.frame( x = seq(from = 3*-2, to = 3*-1, by = 0.1),
                                 y = dnorm(x = seq(from = 3*-2, to = 3*-1, by = 0.1), 
                                           mean = 0, sd = 3) ), 
              aes(ymin = 0, ymax = y), fill=plot_fg_alt, alpha="0.5") + 
  geom_ribbon(data = data.frame( x = seq(from = 3*1, to = 3*2, by = 0.1),
                                 y = dnorm(x = seq(from = 3*1, to = 3*2, by = 0.1), 
                                           mean = 0, sd = 3) ), 
              aes(ymin = 0, ymax = y), fill=plot_fg_alt, alpha="0.5") + 
  geom_segment(aes(x = 3*-2, xend = 3*2, y = 0.05/3, yend = 0.05/3),
               lty = "dashed", size = 0.2, 
               arrow = arrow(ends = "both",
                             length = unit(0.15, "inches"))) +
  geom_label(aes(label = "95%", x = 0, y = 0.05/3, size = label_font_size), col = "black",
             label.size = 0, label.padding = unit(0.1,"lines"), 
             fill = plot_bg) +
  scale_size_identity() + 
  scale_y_continuous(breaks = NULL) + 
  scale_x_continuous(breaks = 3*c(-3,-2,-1,0,1,2,3) ) +
  labs(title = "Normal Distribution - Spread of 3",
       x = "Z Score",
       y = "Density") + 
  theme_classic() + 
  theme(plot.title = element_text(hjust = 0.5, face="bold", 
                                  margin = margin(10, 0, 10, 0),
                                  colour = plot_fg),
        legend.justification=c(0,1), 
        legend.position=c(0,1), 
        legend.title = element_blank(),
        legend.key = element_rect(fill = plot_bg, color = plot_bg),
        legend.background = element_rect(fill = plot_bg, color = plot_bg),
        legend.box.background = element_rect(fill = plot_bg, color = plot_bg),
        legend.text = element_text(colour = plot_fg),
        text = element_text(size=16), 
        panel.border = element_blank(), 
        panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(), 
        panel.background = element_rect(fill = plot_bg, color = plot_bg),
        plot.background = element_rect(fill = plot_bg, color = plot_bg),
        axis.line = element_line(colour = plot_bg),
        axis.title.x = element_text(margin = unit(c(4, 0, 0, 0), "mm"), 
                                    colour = plot_fg),
        axis.title.y = element_text(margin = unit(c(0, 4, 0, 0), "mm"), 
                                    angle = 90, colour = plot_fg) )
```

That is indeed what we see when we draw the picture. Just like with payoff diagrams, I'd strongly encourage you to draw normal distributions when we encounter them with options. Visualizing a distribution is the best way to understand what they say about future outcomes. Can you draw a normal distribution centered at 5 with a spread of 3? It should have 68& of its values between 2 and 8 and 95% of its values between -1 and 11. 

If we're being mathematical, the center of a normal distribution is called its *mean* and its spread is called its *standard deviation*. For the normal distribution, these words literally mean what we've calling them so far. The *mean* is its center, and the *standard deviation* measures how spread out it is by specifying the distance we have move from the center to capture 68% of the future outcomes. I'll soon be calling the spread by another name: *implied volatility*.


### Implied Volatility Redux

We've almost built up enough machinery to say what the implied volatility is. The next thing we need to do is make some assumptions on the behavior of markets and market prices, so that we can use a probability distribution to describe the unknown returns we'll receive in the future. The Black-Scholes model makes a number of assumptions, many of them unrealistic. I'll quickly go through some of the assumptions and their implications, but I'm not that interested in the specifics of the model. 

One of the assumptions is that we cannot make money from risk-free trades. This is the idea behind *replicating portfolios*. If we can create two portfolios with the same payoff diagram, they must have the same price. Otherwise, we could sell one and buy the other to make free money. Another assumption is that we pay no fees to the bank when we trade, which is becoming more true over time. A third assumption is that stock prices follow something called a geometric Brownian motion. This assumption leads to an option pricing formula in terms of the five components we described at the beginning of this chapter. 

The third assumption implies that stock prices are log-normally distributed with *implied volatility* $\sigma$. This gives a great idea about what *implied volatility* is - it's the market's opinion of the spread of the stock over the next year. At this stage, you might ask why *implied volatility* cannot be directly observed by market prices. We could calculate the spread for the last year, but this approach has a serious shortcoming - it assumes the future (next year) will be exactly like the past (last year). That's very unlikely to be true. If it was, we wouldn't need to do much figure out what stocks to invest in. 

So we can't observe the *implied volatility* from market prices - it has to be determined by a mathematical model. Black-Scholes is just one such model. Since the Black-Scholes model was introduced, many more option pricing models have been developed. They seek to make more realistic modeling assumptions and price options more accurately. 

-->